{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Resources for scraping Reddit:***\n",
    "    \n",
    "PRAW: https://towardsdatascience.com/scraping-reddit-data-1c0af3040768\n",
    "\n",
    "PUSHSHIFT: https://medium.com/@RareLoot/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import praw\n",
    "import pprint\n",
    "from reddit_credentials import reddit_info\n",
    "from praw.models import MoreComments\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging in to use Python Reddit API\n",
    "\n",
    "reddit = praw.Reddit(client_id=reddit_info['client_id'],\n",
    "                     client_secret=reddit_info['client_secret'],\n",
    "                     user_agent=reddit_info['user_agent'],\n",
    "                     username=reddit_info['username'],\n",
    "                     password=reddit_info['password']\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convo with Aaron and Beatriz\n",
    "\n",
    "- how has it changed over time in terms of positive vs negative sentiment? sentiment analysis\n",
    "- what are people saying? how do tey feel about it? \n",
    "- can I group user groups together? \n",
    "- can I group certain topics together?\n",
    "    - are people talking predominantly about carbon emissions? or that its a hoax? etc.\n",
    "    - there are anumber of questions about \"what is even happening\" tthat need to be answered, and unsupervised learning will help?\n",
    "\n",
    "- can do miultiple methods at once\n",
    "    - classify topics\n",
    "    - then do some sentiment analysis - whether or not a certain post is more leaning towards negative vs positive outlook on this\n",
    "- do everything they talked about in class\n",
    "    - use word embedding ever which way\n",
    "    - use topic modeling every which way\n",
    "    - use everythign every way they talk about it\n",
    "\n",
    "- stop words\n",
    "- get rid of punctuation\n",
    "- lower case all\n",
    "- functions to tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYT API\n",
    "# GDELT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas from convo with Kelly\n",
    "\n",
    "- sentiment & topics of climate change discussions\n",
    "    - when people talk about climate change, what is the tone of the conversation? (sentiment analysis)\n",
    "    - topic modeling\n",
    "- look at different patterns in way people talk about climate change\n",
    "- how conversation has changed over time, if I can get longitudinal data out of this\n",
    "- depending on what I get out of that, find interesting patterns\n",
    "    - may stimulate further ideas down the road\n",
    " \n",
    "- I can engineer my own features\n",
    "    - look for specific words (word tokens) like \"not sure\" to see how people are equivocating about climate change\n",
    "        - create functions that go through each of my reddit comments and look for certain words or things I think might be relevant to some of the data\n",
    "\n",
    "\n",
    "- both of the below topics involve how people frame arguments; so what are the essential ways people frame arguments for reproductive rights or global warming\n",
    "\n",
    "\n",
    "***abortion***\n",
    "- could turn into a predictive model -- given the way people are talking about the issue here, are they prolife or prochoice? \n",
    "    - because it's kind of like I have labeled data here based on subreddit names\n",
    "- George Lakoff -- gets into psychology and the way people with different political views frame arguments\n",
    "    - could look at things like sentiment, word frequency (whether a word is high vs. low frequency word)\n",
    "        - can you assume some level of education from the text based on how people are using language?\n",
    "            - by looking at vocabulary; TF-IDF is important here; looking at weight of each word\n",
    "                - if people are using v low freuqency words (10 cent words) in their speach, can assume they are more highly educated\n",
    "                - start to make some hypothesis around how people with dif political views might use language, so when doing NLP can engineer some features to let me know whether or not someone is for or against reproductive rights\n",
    "                \n",
    "- reddit aggregator that gives individual info about each reddit commenter (Kelly can figure out name and send it to me)\n",
    "    - from there can get info about what each of the reddit user likes and what other subreddits they belong to\n",
    "    - might have some geolocation data (so I know where in the country the person is located who makes certain comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dan's question: SVD, PCA, LDA -- when to use each in project flow?\n",
    "\n",
    "- 10,000 comments form dif subreddits\n",
    "- reduce dataset\n",
    "    - get rid of stopwords etc.\n",
    "- once have countvectorizer or ?, do some dimensionality reduction on it\n",
    "- we do truncated SVD (e.g., chop down to 10-20 principle components)\n",
    "    - LDA would do somethign similar, but SVD is default\n",
    "    - reduce to 15 themes in latent space; some sort of cocktail mixture of those 15 themes\n",
    "    - lets also track the subreddits they came from and see the purity of the subreddits\n",
    "        - in a perfect world, one subreddit would go to each theme, but there prob will be some crossover\n",
    "            - see what percentage of each subreddit goes to each theme?\n",
    "- could also try various forms of clusering on raw dataset or the reduced dataset, and see what the clustering says\n",
    "    - what does DBSCAN say? or what does spectral clustering say? spectral clustering is combo of ?? and kmeans - reduces dimensions and then does kmeans to cluster documents together\n",
    "- clustering as preliminary baseline to get an idea; then do something like LDA\n",
    "    - LDA will give you things like top 10 words in first topic, top 10 words in 2nd topic, etc.\n",
    "    - then do things like LDA in wider buckets, smaller buckets, etc. (?)\n",
    "    - note - you have the subbreddit topic sthey come from; don't lose that order so yo uknow where they come from\n",
    "        - just keep away from any modeling you do so we can see how a blind unsupervsed model would do\n",
    "            - maybe we find out that the human labeling isn't that great and the model figures out a better distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how has sentiment / topics changed over time with abortion topic?\n",
    "# if date is convenient, could be interesting, if not, don't worry about it. \n",
    "# so can use praw?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# collecting data using pushshift\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "title_list = []\n",
    "\n",
    "files = ['may_2015', 'june_2015', 'july_2015', 'august_2015', 'september_2015', \n",
    "        'october_2015', 'november_2015', 'december_2015', 'january_2016', \n",
    "        'february_2016', 'march_2016', 'april_2016', 'may_2016', 'june_2016',\n",
    "        'july_2016', 'august_2016', 'september_2016', 'october_2016', \n",
    "        'november_2016', 'december_2016', 'january_2017', 'february_2017',\n",
    "        'march_2017', 'april_2017', 'may_2017', 'june_2017', 'july_2017', \n",
    "        'august_2017', 'september_2017', 'october_2017', 'november_2017', \n",
    "        'december_2017', 'january_2018', 'february_2018', 'march_2018', \n",
    "        'april_2018', 'may_2018', 'june_2018', 'july_2018', 'august_2018', \n",
    "        'september_2018', 'october_2018', 'november_2018', 'december_2018', \n",
    "        'january_2019', 'february_2019', 'march_2019', 'april_2019']      \n",
    "\n",
    "for file in files:\n",
    "    path = 'data/' + file + '.json'\n",
    "    with open(path) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        for p in data['data']:\n",
    "            title_list.append(p['title'])\n",
    "\n",
    "print(len(set(title_list)))\n",
    "df = pd.DataFrame(set(title_list))\n",
    "df.to_pickle('data/posts.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## LEARNING TO USE PRAW\n",
    "# get 10 hot posts from the prochoice subreddit\n",
    "\n",
    "hot_posts = reddit.subreddit('prochoice').hot(limit=10)\n",
    "for post in hot_posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## LEARNING TO USE PRAW\n",
    "# This variable can be iterated over \n",
    "# and features including the post title, id and url can be extracted and saved into an .csv file.\n",
    "\n",
    "posts = []\n",
    "pc_subreddit = reddit.subreddit('prochoice')\n",
    "for post in pc_subreddit.hot(limit=10):\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "posts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get comments from a specific post\n",
    "\n",
    "# submission = reddit.submission(url=\"https://www.reddit.com/r/MapPorn/comments/a3p0uq/an_image_of_gps_tracking_of_multiple_wolves_in/\")\n",
    "# or \n",
    "submission = reddit.submission(id=\"83863l\") # create submission object\n",
    "\n",
    "for top_level_comment in submission.comments: # get top level comments\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# if get error for above becuase there are More Comments:\n",
    "\n",
    "from praw.models import MoreComments # get top level comments 2\n",
    "for top_level_comment in submission.comments:\n",
    "    if isinstance(top_level_comment, MoreComments):\n",
    "        continue\n",
    "    print(top_level_comment.body)\n",
    "\n",
    "\n",
    "submission.comments.replace_more(limit=0) # get top level comments 3\n",
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# get all comments inside the comment section (comment forest)\n",
    "\n",
    "submission.comments.replace_more(limit=0)\n",
    "for comment in submission.comments.list():\n",
    "    print(comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at general information about the prochoice and prolife subreddits\n",
    "# to find other similar subreddits to add to NLP analysis\n",
    "\n",
    "pc_subreddit = reddit.subreddit('prochoice')\n",
    "pl_subreddit = reddit.subreddit('prolife')\n",
    "\n",
    "\n",
    "print(pc_subreddit.description)\n",
    "print(pl_subreddit.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object name for subreddit forum of interest\n",
    "# climate = reddit.subreddit('climate change')\n",
    "\n",
    "# subreddit_list = ['GlobalWarming','climate','climatechange','climateskeptics']\n",
    "\n",
    "prolife_subreddits = ['trueprolife', 'abortiondebate', 'gendercide', 'Adoption', 'Fosterit', 'ProLifeLibertarians']\n",
    "\n",
    "prochoice_subreddits = ['abortion', 'abortions', 'abortiondebate', 'childfree', 'feminism101', 'Feminism4Everyone', \n",
    "                        'feministFAQ', 'GenderStudies', 'InternationalWomen', 'LiberalFeminism', \n",
    "                        'LibertarianFeminism', 'libs', 'onlywomen', 'riotgrrrl', 'SecondWaveFeminism', \n",
    "                        'thetruefeminism', 'ThirdWaveFeminism', 'WomenPositive', 'zines']\n",
    "\n",
    "other_subreddits = ['TwoXChromosomes, Feminism, antifeminists']\n",
    "\n",
    "subreddit_list = ['prochoice', 'prolife', 'trueprolife', 'abortion', 'abortions', 'abortiondebate'] \n",
    "\n",
    "\n",
    "# d = {\"subreddit\":[],\n",
    "#      \"title\":[],\n",
    "#      \"score\":[],\n",
    "#      \"id\":[],\n",
    "#      \"url\":[],\n",
    "#      \"comms_num\": [],\n",
    "#      \"created\": [],\n",
    "#      \"body\":[],\n",
    "#      \"comments\":[]\n",
    "#     }\n",
    "\n",
    "\n",
    "d = {\"subreddit\":[],\n",
    "     \"title\":[],\n",
    "     \"comments\":[]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subreddit_name in subreddit_list:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    for submission in subreddit.top(limit=3): # Dan is getting top 50-200 at most\n",
    "        d[\"subreddit\"].append(subreddit)\n",
    "        d[\"title\"].append(submission.title)\n",
    "        #d[\"score\"].append(submission.score)\n",
    "        #d[\"id\"].append(submission.id)\n",
    "        #d[\"url\"].append(submission.url)\n",
    "        #d[\"comms_num\"].append(submission.num_comments)\n",
    "        #d[\"created\"].append(submission.created)\n",
    "        #d[\"body\"].append(submission.selftext)\n",
    "        submission.comments.replace_more(limit=2) # could extend that limit to get all the comments in comment tree\n",
    "        d[\"comments\"].append(submission.comments)\n",
    "#         for comment in (submission.comments.list()):\n",
    "#             print(comment.body)\n",
    "        \n",
    "#df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(d, open('reproductive_rights_comments.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pickle.load(open('reproductive_rights_comments.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(d['subreddit']).keys())\n",
    "(Counter(d['subreddit']).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "for i,forest in enumerate(d['comments']):\n",
    "    for comment in forest.list():\n",
    "        comments.append([comment.body, str(d['subreddit'][i])])\n",
    "\n",
    "comments_only = []\n",
    "for comment in comments:\n",
    "    comments_only.append(comment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = []\n",
    "coms = [] # in data cleaning file, make all lowercase so its consistent\n",
    "\n",
    "for i,forest in enumerate(d['comments']):\n",
    "    for comment in forest.list():\n",
    "        coms.append([comment.body])\n",
    "        subs.append(str(d['subreddit'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
